{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym\n",
    "\n",
    "- 目录\n",
    "    - 任务目标\n",
    "    - 算法依赖的库\n",
    "    - Replay Memory\n",
    "    - DQN algorithm\n",
    "\n",
    "### 任务目标\n",
    "\n",
    "- Agent 必须在两种动作中做出选择（向左或向右移动手推车）这样连接在手推车上的杆子才能保持直立。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Packages\n",
    "\n",
    "- 首先我们需要 gym 环境:\n",
    "    `pip install gym`\n",
    "\n",
    "- 和其他关于 pytorch 的库:\n",
    "    - neural networks (torch.nn)\n",
    "    - optimization (torch.optim)\n",
    "    - automatic differentiation (torch.autograd)\n",
    "    - utilities for vision tasks (torchvision - a separate package)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Replay Memory\n",
    "\n",
    "- `Transition` 表示环境中单个转换的命名元组。\n",
    "它实际上将 `(state, action)` 对映射到它们的 `(next_state, reward)` 结果 (状态是屏幕差异图像).\n",
    "\n",
    "- `ReplayMemory` 一种大小有界的循环缓冲区，用于保存最近观察到的 `Transitions`。\n",
    "它还实现了一个 `.sample()` 方法，用于选择用于训练的随机一批 `Transitions`。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DQN algorithm\n",
    "\n",
    "我们的目标将是训练一种试图最大化累积回报(reward)的政策(policy) :\n",
    "\n",
    "$$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$$\n",
    "\n",
    "where $R_{t_0}$ 是返回值, 折现率 $\\gamma$ 应该是一个介于0和1之间的常量，以确保和收敛.\n",
    "对我们的Agent来说，不确定的遥远未来的回报比我们相当有信心的不久的将来的回报更重要.\n",
    "\n",
    "Q-learning 背后的主要思想是, 如果我们有一个函数 $Q^*: State \\times Action \\rightarrow \\mathbb{R}$ ,\n",
    "能告诉我们得到的是什么, 如果我们在一个给定的状态(state)下, 要执行一个行为(action), \n",
    "然后我们可以很容易地构造一个策略(policy), 以最大化回报(reward) :\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)$$\n",
    "\n",
    "然而, 我们无法知晓真实世界下的所有情况, 所以我们不能直接求的$Q^*$,\n",
    "但是, 由于神经网络可以看作通用函数逼近器, 可以利用神经网络训练并近似$Q^*$ :\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)$$\n",
    "\n",
    "训练更新规则是 , 对于某些策略(policy), 每个 $Q$ 函数都遵守 Bellman 方程 :\n",
    "\n",
    "$$Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))$$\n",
    "\n",
    "等式两边的差称为 时序差分误差(temporal difference error), $\\delta$ :\n",
    "\n",
    "$$\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))$$\n",
    "\n",
    "为了最小化这个误差, 我们使用 `Huber loss`. \n",
    "\n",
    "> 当误差很小的时候, `Huber loss`就像均方误差(MSE), 但当误差很大的时候, 就像绝对平均误差(MAE), 当 $Q$ 的近似有很大的噪声时，这使得它对离群值更加鲁棒.\n",
    "\n",
    "我们通过从Replay Memory中采样的一批Transition来计算:\n",
    " \n",
    "$$\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)$$\n",
    "\n",
    "$$\n",
    "\\begin{split}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "  \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "  |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "\\end{cases}\\end{split}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            \"\"\"conv2d layer output size\"\"\"\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Input extraction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}