{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym\n",
    "\n",
    "### OpenAI Gym 中的基本概念\n",
    "\n",
    "如果算法想做得更好, 而不是在每一步都采取随机的行动, 只有真正了解算法的行为对环境的影响才可能会更好.\n",
    "\n",
    "环境(environment)的 `step` 函数返回的正是我们所需要的. 实际上, `step` 返回四个值. 这些都是:\n",
    "\n",
    "- observation: 特定于环境的对象，表示您对环境的观察. 例如，来自摄像机的像素数据, 机器人的关节角度和关节速度, 或棋盘游戏中的棋盘状态.\n",
    "\n",
    "- reward: 前一个动作所获得的奖励量. 在不同的环境中, 这个范围是不同的, 但是目标总是要增加总回报。\n",
    "\n",
    "- done: 重新设置(reset)环境的标志.\n",
    "大多数(但不是所有)任务被划分为定义良好的场景(episodes), done为真表示场景已经终止.(例如, 也许杆子倒得太远了.)\n",
    "\n",
    "- info: 对调试有用的诊断信息. 它有时对学习很有用(例如，它可能包含环境最后一次状态更改后的原始概率). \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img style=\"border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\"\n",
    "    height=\"150\"\n",
    "    src=\"https://github.com/ximingxing/Images/raw/master/dlic/agent-environment%20loop.svg\">\n",
    "    <br>\n",
    "    <div style=\"border-bottom: 1px solid #d9d9d9;display:inline-block;color: #999;padding: 2px;\n",
    "    font-style: oblique; font-family: 'Times New Roman'\">\n",
    "    Figure: Agent-Environment Loop</div>\n",
    "</div>\n",
    "\n",
    "###  CartPole-v0 task 任务目标\n",
    "\n",
    "- Agent(决策者 -- model)必须在两种动作(action)中做出选择（向左或向右移动手推车）这样连接在手推车上的杆子才能保持直立。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Packages\n",
    "\n",
    "- 首先我们需要 gym 环境:\n",
    "    `pip install gym`\n",
    "\n",
    "- 和其他关于 pytorch 的库:\n",
    "    - neural networks (torch.nn)\n",
    "    - optimization (torch.optim)\n",
    "    - automatic differentiation (torch.autograd)\n",
    "    - utilities for vision tasks (torchvision - a separate package)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Replay Memory\n",
    "\n",
    "- `Transition` 表示环境中单个转换的命名元组。\n",
    "它实际上将 `(state, action)` 对映射到它们的 `(next_state, reward)` 结果 (状态是屏幕差异图像).\n",
    "\n",
    "- `ReplayMemory` 一种大小有界的循环缓冲区，用于保存最近观察到的 `Transitions`。\n",
    "它还实现了一个 `.sample()` 方法，用于选择用于训练的随机一批 `Transitions`.\n",
    "\n",
    "#### 作用\n",
    "\n",
    "1. 储存过去遇到的 `Transition`\n",
    "2. 忘记太过久远的 `Transition`\n",
    "3. 训练时, 从 `ReplayMemory` 随机抽样"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DQN algorithm\n",
    "\n",
    "我们的目标将是训练一种试图最大化累积回报(reward)的政策(policy) :\n",
    "\n",
    "$$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$$\n",
    "\n",
    "where $R_{t_0}$ 是返回值, 递减因子 $\\gamma$ 应该是一个介于0和1之间的常量，以确保和收敛.\n",
    "\n",
    "Q-learning 背后的主要思想是, 如果我们有一个函数 $Q^*: State \\times Action \\rightarrow \\mathbb{R}$ ,\n",
    "能告诉我们得到的是什么, 如果我们在一个给定的状态(state)下, 要执行一个行为(action), \n",
    "然后我们可以很容易地构造一个策略(policy), 以最大化回报(reward) :\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)$$\n",
    "\n",
    "然而, 我们无法知晓真实世界下的所有情况, 所以我们不能直接求的$Q^*$,\n",
    "但是, 由于神经网络可以看作通用函数逼近器, 可以利用神经网络训练并近似$Q^*$ :\n",
    "\n",
    "训练更新规则是 , 对于某些策略(policy), 每个 $Q$ 函数都遵守 Bellman 方程 :\n",
    "\n",
    "$$Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))$$\n",
    "\n",
    "这里 $r$ 表示当前状态下的奖励, \n",
    "$Q^{\\pi}(s', \\pi(s'))$ 表示神经网络预测后续若干个状态中执行action对应奖励,\n",
    "$\\gamma$ 代表第二项递减因子, 由于神经网络预测距离当前state越远的reward可能是越不准确的, \n",
    "等式两边的差称为 时序差分误差(temporal difference error), $\\delta$ :\n",
    "\n",
    "$$\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))$$\n",
    "\n",
    "为了最小化这个误差, 我们使用 `Huber loss`. \n",
    "\n",
    "> 当误差很小的时候, `Huber loss`就像均方误差(MSE), 但当误差很大的时候, 就像绝对平均误差(MAE), 当 $Q$ 的近似有很大的噪声时，这使得它对离群值更加鲁棒.\n",
    "\n",
    "我们通过从Replay Memory中采样的一批Transition来计算:\n",
    " \n",
    "$$\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)$$\n",
    "\n",
    "$$\n",
    "\\begin{split}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "  \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "  |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "\\end{cases}\\end{split}\n",
    "$$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img style=\"border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\"\n",
    "    height=\"200\"\n",
    "    src=\"https://github.com/ximingxing/Images/raw/master/dlic/reinforcement_learning_diagram.jpg\">\n",
    "    <br>\n",
    "    <div style=\"border-bottom: 1px solid #d9d9d9;display:inline-block;color: #999;padding: 2px;\n",
    "    font-style: oblique; font-family: 'Times New Roman'\">\n",
    "    Figure: Reinforcement Learning Diagram</div>\n",
    "</div>\n",
    "\n",
    "行为(action)可以随机选择, 也可以基于策略(policy), 从`Gym`环境(environment)中采样下一步的样本。\n",
    "我们将结果(result)记录在回放内存(replay memory)中，并在每次迭代中运行优化步骤。\n",
    "优化从回放内存(replay memory)中随机挑选一些批次(batch)来进行新策略(policy)的训练。\n",
    "Target net也用于优化计算期望Q的值;它偶尔会更新以保持最新。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            \"\"\"conv2d layer output size\"\"\"\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Input extraction\n",
    "\n",
    "下面的代码用于从环境(environment)中提取和处理呈现图像 (基于`torchvision`库)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "#### Hyperparameters and utilities\n",
    "\n",
    "- `select_action` 将根据贪心策略选择相应的行为(action). 简单地说, 我们有时会用我们的模型来选择行为, 有时我们会统一地选取一个.\n",
    "选择随机动作的概率将从 `EPS_START` 开始，并以指数方式衰减到 `EPS_END`. `EPS_DECAY` 控制衰变的速率.\n",
    "\n",
    "- `plot_durations` 帮助绘制场景(episodes)的时长，以及最近100个场景(episodes)的平均时长."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training loop\n",
    "\n",
    "在这里, `optimize_model` 函数，它执行优化的单个步骤.\n",
    "它首先对一批张量进行采样(state, action, reward)，将所有的张量串联成一个张量, \n",
    "然后分别计算 $Q(s_t, a_t)$ 和 $V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, 将其合并为训练损失(loss).\n",
    "当状态变为为终止状态(terminal state)时, 对应 $V(s) = 0$,\n",
    "我们还使用一个目标网络(target network)来计算 $V(s_{t+1})$ 以增加稳定性.\n",
    "\n",
    "> 目标网络(target network)的权值在大部分时间内保持不变，但时不时地会更新策略网络(policy network)的权值, \n",
    "> 这里用一个情节(episodes)表示."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面是主要的训练循环.\n",
    "在开始我们重置环境和初始化状态(state)张量.\n",
    "然后，我们对一个动作(action)采样, 执行它, 观察下一个屏幕(screen)和奖励(reward), 并优化我们的模型一次.\n",
    "当情节(episode)结束(我们的模型失败), 我们重新启动循环."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": [
      "Complete\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}