{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## GCN node classification based on Cora dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import os.path as osp\n",
    "import pickle\n",
    "import urllib\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data processing(dataset: cora)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Data = namedtuple('Data', ['x', 'y', 'adjacency',\n",
    "                           'train_mask', 'val_mask', 'test_mask'])\n",
    "\n",
    "\n",
    "def tensor_from_numpy(x, device):\n",
    "    return torch.from_numpy(x).to(device)\n",
    "\n",
    "\n",
    "class CoraData(object):\n",
    "    download_url = \"https://github.com/kimiyoung/planetoid/raw/master/data\"\n",
    "    filenames = [\"ind.cora.{}\".format(name) for name in\n",
    "                 ['x', 'tx', 'allx', 'y', 'ty', 'ally', 'graph', 'test.index']]\n",
    "\n",
    "    def __init__(self, data_root=\"cora\", rebuild=False):\n",
    "        \"\"\"Cora数据，包括数据下载，处理，加载等功能\n",
    "        当数据的缓存文件存在时，将使用缓存文件，否则将下载、进行处理，并缓存到磁盘\n",
    "\n",
    "        处理之后的数据可以通过属性 .data 获得，它将返回一个数据对象，包括如下几部分：\n",
    "            * x: 节点的特征，维度为 2708 * 1433，类型为 np.ndarray\n",
    "            * y: 节点的标签，总共包括7个类别，类型为 np.ndarray\n",
    "            * adjacency: 邻接矩阵，维度为 2708 * 2708，类型为 scipy.sparse.coo.coo_matrix\n",
    "            * train_mask: 训练集掩码向量，维度为 2708，当节点属于训练集时，相应位置为True，否则False\n",
    "            * val_mask: 验证集掩码向量，维度为 2708，当节点属于验证集时，相应位置为True，否则False\n",
    "            * test_mask: 测试集掩码向量，维度为 2708，当节点属于测试集时，相应位置为True，否则False\n",
    "\n",
    "        Args:\n",
    "        -------\n",
    "            data_root: string, optional\n",
    "                存放数据的目录，原始数据路径: {data_root}/raw\n",
    "                缓存数据路径: {data_root}/processed_cora.pkl\n",
    "            rebuild: boolean, optional\n",
    "                是否需要重新构建数据集，当设为True时，如果存在缓存数据也会重建数据\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_root = data_root\n",
    "        save_file = osp.join(self.data_root, \"processed_cora.pkl\")\n",
    "        if osp.exists(save_file) and not rebuild:\n",
    "            print(\"Using Cached file: {}\".format(save_file))\n",
    "            self._data = pickle.load(open(save_file, \"rb\"))\n",
    "        else:\n",
    "            self.maybe_download()\n",
    "            self._data = self.process_data()\n",
    "            with open(save_file, \"wb\") as f:\n",
    "                pickle.dump(self.data, f)\n",
    "            print(\"Cached file: {}\".format(save_file))\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"返回Data数据对象，包括x, y, adjacency, train_mask, val_mask, test_mask\"\"\"\n",
    "        return self._data\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"\n",
    "        处理数据，得到节点特征和标签，邻接矩阵，训练集、验证集以及测试集\n",
    "        引用自：https://github.com/rusty1s/pytorch_geometric\n",
    "        \"\"\"\n",
    "        print(\"Process data ...\")\n",
    "        _, tx, allx, y, ty, ally, graph, test_index = [self.read_data(\n",
    "            osp.join(self.data_root, \"raw\", name)) for name in self.filenames]\n",
    "        train_index = np.arange(y.shape[0])\n",
    "        val_index = np.arange(y.shape[0], y.shape[0] + 500)\n",
    "        sorted_test_index = sorted(test_index)\n",
    "\n",
    "        x = np.concatenate((allx, tx), axis=0)\n",
    "        y = np.concatenate((ally, ty), axis=0).argmax(axis=1)\n",
    "\n",
    "        x[test_index] = x[sorted_test_index]\n",
    "        y[test_index] = y[sorted_test_index]\n",
    "        num_nodes = x.shape[0]\n",
    "\n",
    "        train_mask = np.zeros(num_nodes, dtype=np.bool)\n",
    "        val_mask = np.zeros(num_nodes, dtype=np.bool)\n",
    "        test_mask = np.zeros(num_nodes, dtype=np.bool)\n",
    "        train_mask[train_index] = True\n",
    "        val_mask[val_index] = True\n",
    "        test_mask[test_index] = True\n",
    "        adjacency = self.build_adjacency(graph)\n",
    "        print(\"Node's feature shape: \", x.shape)\n",
    "        print(\"Node's label shape: \", y.shape)\n",
    "        print(\"Adjacency's shape: \", adjacency.shape)\n",
    "        print(\"Number of training nodes: \", train_mask.sum())\n",
    "        print(\"Number of validation nodes: \", val_mask.sum())\n",
    "        print(\"Number of test nodes: \", test_mask.sum())\n",
    "\n",
    "        return Data(x=x, y=y, adjacency=adjacency,\n",
    "                    train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "    def maybe_download(self):\n",
    "        save_path = os.path.join(self.data_root, \"raw\")\n",
    "        for name in self.filenames:\n",
    "            if not osp.exists(osp.join(save_path, name)):\n",
    "                self.download_data(\n",
    "                    \"{}/{}\".format(self.download_url, name), save_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_adjacency(adj_dict):\n",
    "        \"\"\"根据邻接表创建邻接矩阵\"\"\"\n",
    "        edge_index = []\n",
    "        num_nodes = len(adj_dict)\n",
    "        for src, dst in adj_dict.items():\n",
    "            edge_index.extend([src, v] for v in dst)\n",
    "            edge_index.extend([v, src] for v in dst)\n",
    "        # 去除重复的边\n",
    "        edge_index = list(k for k, _ in itertools.groupby(sorted(edge_index)))\n",
    "        edge_index = np.asarray(edge_index)\n",
    "        adjacency = sp.coo_matrix((np.ones(len(edge_index)), \n",
    "                                   (edge_index[:, 0], edge_index[:, 1])),\n",
    "                    shape=(num_nodes, num_nodes), dtype=\"float32\")\n",
    "        return adjacency\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(path):\n",
    "        \"\"\"使用不同的方式读取原始数据以进一步处理\"\"\"\n",
    "        name = osp.basename(path)\n",
    "        if name == \"ind.cora.test.index\":\n",
    "            out = np.genfromtxt(path, dtype=\"int64\")\n",
    "            return out\n",
    "        else:\n",
    "            out = pickle.load(open(path, \"rb\"), encoding=\"latin1\")\n",
    "            out = out.toarray() if hasattr(out, \"toarray\") else out\n",
    "            return out\n",
    "\n",
    "    @staticmethod\n",
    "    def download_data(url, save_path):\n",
    "        \"\"\"数据下载工具，当原始数据不存在时将会进行下载\"\"\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        data = urllib.request.urlopen(url)\n",
    "        filename = os.path.split(url)[-1]\n",
    "\n",
    "        with open(os.path.join(save_path, filename), 'wb') as f:\n",
    "            f.write(data.read())\n",
    "\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def normalization(adjacency):\n",
    "        \"\"\"计算 L=D^-0.5 * (A+I) * D^-0.5\"\"\"\n",
    "        adjacency += sp.eye(adjacency.shape[0])    # 增加自连接\n",
    "        degree = np.array(adjacency.sum(1))\n",
    "        d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "        return d_hat.dot(adjacency).dot(d_hat).tocoo()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define gcn layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
    "        \"\"\"图卷积：L*X*\\theta\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "            input_dim: int\n",
    "                节点输入特征的维度\n",
    "            output_dim: int\n",
    "                输出特征维度\n",
    "            use_bias : bool, optional\n",
    "                是否使用偏置\n",
    "        \"\"\"\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.use_bias = use_bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight)\n",
    "        if self.use_bias:\n",
    "            init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, adjacency, input_feature):\n",
    "        \"\"\"邻接矩阵是稀疏矩阵，因此在计算时使用稀疏矩阵乘法\n",
    "    \n",
    "        Args: \n",
    "        -------\n",
    "            adjacency: torch.sparse.FloatTensor\n",
    "                邻接矩阵\n",
    "            input_feature: torch.Tensor\n",
    "                输入特征\n",
    "        \"\"\"\n",
    "        support = torch.mm(input_feature, self.weight)\n",
    "        output = torch.sparse.mm(adjacency, support)\n",
    "        if self.use_bias:\n",
    "            output += self.bias\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "            + str(self.input_dim) + ' -> ' \\\n",
    "            + str(self.output_dim) + ')'\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    定义一个包含两层GraphConvolution的模型\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1433):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gcn1 = GraphConvolution(input_dim, 16)\n",
    "        self.gcn2 = GraphConvolution(16, 7)\n",
    "    \n",
    "    def forward(self, adjacency, feature):\n",
    "        h = F.relu(self.gcn1(adjacency, feature))\n",
    "        logits = self.gcn2(adjacency, h)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "WEIGHT_DACAY = 5e-4\n",
    "EPOCHS = 20\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 加载数据，并转换为torch.Tensor\n",
    "dataset = CoraData().data\n",
    "node_feature = dataset.x / dataset.x.sum(1, keepdims=True)  # 归一化数据，使得每一行和为1\n",
    "tensor_x = tensor_from_numpy(node_feature, DEVICE)\n",
    "tensor_y = tensor_from_numpy(dataset.y, DEVICE)\n",
    "tensor_train_mask = tensor_from_numpy(dataset.train_mask, DEVICE)\n",
    "tensor_val_mask = tensor_from_numpy(dataset.val_mask, DEVICE)\n",
    "tensor_test_mask = tensor_from_numpy(dataset.test_mask, DEVICE)\n",
    "normalize_adjacency = CoraData.normalization(dataset.adjacency)   # 规范化邻接矩阵\n",
    "\n",
    "num_nodes, input_dim = node_feature.shape\n",
    "indices = torch.from_numpy(np.asarray([normalize_adjacency.row, \n",
    "                                       normalize_adjacency.col]).astype('int64')).long()\n",
    "values = torch.from_numpy(normalize_adjacency.data.astype(np.float32))\n",
    "tensor_adjacency = torch.sparse.FloatTensor(indices, values, \n",
    "                                            (num_nodes, num_nodes)).to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Init model, define optimizer and loss function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 模型定义：Model, Loss, Optimizer\n",
    "model = GCN(input_dim).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=LEARNING_RATE, \n",
    "                       weight_decay=WEIGHT_DACAY)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define training function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train():\n",
    "    loss_history = []\n",
    "    val_acc_history = []\n",
    "    model.train()\n",
    "    train_y = tensor_y[tensor_train_mask]\n",
    "    for epoch in range(EPOCHS):\n",
    "        logits = model(tensor_adjacency, tensor_x)  # 前向传播\n",
    "        train_mask_logits = logits[tensor_train_mask]   # 只选择训练节点进行监督\n",
    "        loss = criterion(train_mask_logits, train_y)    # 计算损失值\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()     # 反向传播计算参数的梯度\n",
    "        optimizer.step()    # 使用优化方法进行梯度更新\n",
    "        train_acc, _, _ = test(tensor_train_mask)     # 计算当前模型训练集上的准确率\n",
    "        val_acc, _, _ = test(tensor_val_mask)     # 计算当前模型在验证集上的准确率\n",
    "        # 记录训练过程中损失值和准确率的变化，用于画图\n",
    "        loss_history.append(loss.item())\n",
    "        val_acc_history.append(val_acc.item())\n",
    "        print(\"Epoch {:03d}: Loss {:.4f}, TrainAcc {:.4}, ValAcc {:.4f}\".format(\n",
    "            epoch, loss.item(), train_acc.item(), val_acc.item()))\n",
    "    \n",
    "    return loss_history, val_acc_history\n",
    "\n",
    "def test(mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor_adjacency, tensor_x)\n",
    "        test_mask_logits = logits[mask]\n",
    "        predict_y = test_mask_logits.max(1)[1]\n",
    "        accuarcy = torch.eq(predict_y, tensor_y[mask]).float().mean()\n",
    "    return accuarcy, test_mask_logits.cpu().numpy(), tensor_y[mask].cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize the training process"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_loss_with_acc(loss_history, val_acc_history):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.plot(range(len(loss_history)), loss_history,\n",
    "             c=np.array([255, 71, 90]) / 255.)\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    ax2 = fig.add_subplot(111, sharex=ax1, frameon=False)\n",
    "    ax2.plot(range(len(val_acc_history)), val_acc_history,\n",
    "             c=np.array([79, 179, 255]) / 255.)\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.yaxis.set_label_position(\"right\")\n",
    "    plt.ylabel('ValAcc')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Training Loss & Validation Accuracy')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss, val_acc = train()\n",
    "test_acc, test_logits, test_label = test(tensor_test_mask)\n",
    "print(\"Test accuarcy: \", test_acc.item())\n",
    "\n",
    "plot_loss_with_acc(loss, val_acc)\n",
    "\n",
    "# 绘制测试数据的TSNE降维图\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE()\n",
    "out = tsne.fit_transform(test_logits)\n",
    "fig = plt.figure()\n",
    "for i in range(7):\n",
    "    indices = test_label == i\n",
    "    x, y = out[indices].T\n",
    "    plt.scatter(x, y, label=str(i))\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}